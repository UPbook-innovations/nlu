{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"aspect_based_ner_sentiment_restaurants.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMaOK1zlx/utJMFRRzUDN5Z"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9ayP-N_Cqr9K"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/named_entity_recognition_(NER)/aspect_based_ner_sentiment_restaurants.ipynb)\n","\n","\n","\n","\n","Automatically detect positive, negative and neutral aspects about restaurants from user reviews. Instead of labelling the entire review as negative or positive, this model helps identify which exact phrases relate to sentiment identified in the review."]},{"cell_type":"code","metadata":{"id":"NqnAGVadANyZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614376517904,"user_tz":-60,"elapsed":67506,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"4c6518b8-bc24-42ee-e3e4-546d96f0f749"},"source":["import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","! pip install nlu pyspark==2.3.4 > /dev/null\n","\n","import nlu\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pyspark==2.4.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/06/29f80e5a464033432eedf89924e7aa6ebbc47ce4dcd956853a73627f2c07/pyspark-2.4.7.tar.gz (217.9MB)\n","\u001b[K     |████████████████████████████████| 217.9MB 70kB/s \n","\u001b[?25hCollecting py4j==0.10.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n","\u001b[K     |████████████████████████████████| 204kB 20.9MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-2.4.7-py2.py3-none-any.whl size=218279465 sha256=37f941c42432e737a6396f822d2bfc63b586e9a35ca673ed38a2c6141ffa743b\n","  Stored in directory: /root/.cache/pip/wheels/34/1f/2e/1e7460f80acf26b08dbb8c53d7ff9e07146f2a68dd5c732be5\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.7 pyspark-2.4.7\n","Looking in indexes: https://test.pypi.org/simple/, https://pypi.org/simple\n","Collecting nlu_test==1.1.3rc2\n","\u001b[?25l  Downloading https://test-files.pythonhosted.org/packages/5c/84/241410ba610c9281afc8e1cffaa352f5ca83fe6e2574f1cfcdf3334dc81f/nlu_test-1.1.3rc2-py3-none-any.whl (158kB)\n","\u001b[K     |████████████████████████████████| 163kB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlu_test==1.1.3rc2) (1.19.5)\n","Collecting dataclasses\n","  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nlu_test==1.1.3rc2) (1.1.5)\n","Collecting spark-nlp<2.8,>=2.7.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/a5/a5130215b43f3bd0e98bd16c471d36dafeab8855ca17789d4927337fa7dc/spark_nlp-2.7.4-py2.py3-none-any.whl (139kB)\n","\u001b[K     |████████████████████████████████| 143kB 8.5MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlu_test==1.1.3rc2) (3.0.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->nlu_test==1.1.3rc2) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nlu_test==1.1.3rc2) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->nlu_test==1.1.3rc2) (1.15.0)\n","Installing collected packages: dataclasses, spark-nlp, nlu-test\n","Successfully installed dataclasses-0.6 nlu-test-1.1.3rc2 spark-nlp-2.7.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"id":"c-9dIJVco9Xf","executionInfo":{"status":"ok","timestamp":1614376583281,"user_tz":-60,"elapsed":132870,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"a9cd0d1a-5f00-4b72-9c0e-d2b7d9e9e791"},"source":["pipe = nlu.load('en.ner.aspect_sentiment')\n","data = 'We loved our Thai-style main which amazing with lots of flavours very impressive for vegetarian. But the service was below average and the chips were too terrible to finish.'\n","df = pipe.predict([data], output_level='chunk')\n","df"],"execution_count":2,"outputs":[{"output_type":"stream","text":["ner_aspect_based_sentiment download started this may take some time.\n","Approximate size to download 21.3 MB\n","[OK!]\n","glove_6B_300 download started this may take some time.\n","Approximate size to download 426.2 MB\n","[OK!]\n","\n","\n","\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word_embeddings</th>\n","      <th>ner_confidence</th>\n","      <th>entities</th>\n","      <th>entities_class</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[[-0.05083499848842621, 0.2482600063085556, -0...</td>\n","      <td>[1.0, 1.0, 1.0, 0.5135999917984009, 0.93879997...</td>\n","      <td>Thai-style main</td>\n","      <td>POS</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[[-0.05083499848842621, 0.2482600063085556, -0...</td>\n","      <td>[1.0, 1.0, 1.0, 0.5135999917984009, 0.93879997...</td>\n","      <td>flavours</td>\n","      <td>POS</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[[-0.05083499848842621, 0.2482600063085556, -0...</td>\n","      <td>[1.0, 1.0, 1.0, 0.5135999917984009, 0.93879997...</td>\n","      <td>vegetarian</td>\n","      <td>POS</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[[-0.05083499848842621, 0.2482600063085556, -0...</td>\n","      <td>[1.0, 1.0, 1.0, 0.5135999917984009, 0.93879997...</td>\n","      <td>service</td>\n","      <td>NEG</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[[-0.05083499848842621, 0.2482600063085556, -0...</td>\n","      <td>[1.0, 1.0, 1.0, 0.5135999917984009, 0.93879997...</td>\n","      <td>chips</td>\n","      <td>NEG</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                word_embeddings  ... entities_class\n","origin_index                                                     ...               \n","0             [[-0.05083499848842621, 0.2482600063085556, -0...  ...            POS\n","0             [[-0.05083499848842621, 0.2482600063085556, -0...  ...            POS\n","0             [[-0.05083499848842621, 0.2482600063085556, -0...  ...            POS\n","0             [[-0.05083499848842621, 0.2482600063085556, -0...  ...            NEG\n","0             [[-0.05083499848842621, 0.2482600063085556, -0...  ...            NEG\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174},"id":"WFtrCQSnp_Ie","executionInfo":{"status":"ok","timestamp":1614376588254,"user_tz":-60,"elapsed":137835,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"233fd26c-0c9e-45d8-fc1c-7c16aad41e86"},"source":["df = pipe.predict([data], output_level='document')\n","df"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\n","\n","\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word_embeddings</th>\n","      <th>ner_confidence</th>\n","      <th>entities_confidence</th>\n","      <th>document</th>\n","      <th>entities</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[[-0.05083499848842621, 0.2482600063085556, -0...</td>\n","      <td>[1.0, 1.0, 1.0, 0.5135999917984009, 0.93879997...</td>\n","      <td>[POS, POS, POS, NEG, NEG]</td>\n","      <td>We loved our Thai-style main which amazing wit...</td>\n","      <td>[Thai-style main, flavours, vegetarian, servic...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                word_embeddings  ...                                           entities\n","origin_index                                                     ...                                                   \n","0             [[-0.05083499848842621, 0.2482600063085556, -0...  ...  [Thai-style main, flavours, vegetarian, servic...\n","\n","[1 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"GCFVSTRKqIgi","executionInfo":{"status":"ok","timestamp":1614376592010,"user_tz":-60,"elapsed":141577,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"efb9945a-4f48-4dbd-f15e-d24b9c56ffe3"},"source":["data = 'We loved our Thai-style main which amazing with lots of flavours very impressive for vegetarian. But the service was below average and the chips were too terrible to finish.'\n","df = pipe.predict([data], output_level='sentence')\n","df"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n","\n","\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word_embeddings</th>\n","      <th>ner_confidence</th>\n","      <th>sentence</th>\n","      <th>entities_confidence</th>\n","      <th>entities</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[[-0.05083499848842621, 0.2482600063085556, -0...</td>\n","      <td>[1.0, 1.0, 1.0, 0.5135999917984009, 0.93879997...</td>\n","      <td>We loved our Thai-style main which amazing wit...</td>\n","      <td>[POS, POS, POS, NEG, NEG]</td>\n","      <td>[Thai-style main, flavours, vegetarian, servic...</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[[-0.05083499848842621, 0.2482600063085556, -0...</td>\n","      <td>[1.0, 1.0, 1.0, 0.5135999917984009, 0.93879997...</td>\n","      <td>But the service was below average and the chip...</td>\n","      <td>[POS, POS, POS, NEG, NEG]</td>\n","      <td>[Thai-style main, flavours, vegetarian, servic...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                word_embeddings  ...                                           entities\n","origin_index                                                     ...                                                   \n","0             [[-0.05083499848842621, 0.2482600063085556, -0...  ...  [Thai-style main, flavours, vegetarian, servic...\n","0             [[-0.05083499848842621, 0.2482600063085556, -0...  ...  [Thai-style main, flavours, vegetarian, servic...\n","\n","[2 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"Yao4hlfyqQNg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614376592012,"user_tz":-60,"elapsed":141571,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"eaa48aab-6fb2-480a-beab-36982127d3d0"},"source":["nlu.print_all_model_kinds_for_action('pos')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["For language <nl> NLU provides the following Models : \n","nlu.load('nl.pos') returns Spark NLP model pos_ud_alpino\n","nlu.load('nl.pos.ud_alpino') returns Spark NLP model pos_ud_alpino\n","For language <en> NLU provides the following Models : \n","nlu.load('en.pos') returns Spark NLP model pos_anc\n","nlu.load('en.pos.anc') returns Spark NLP model pos_anc\n","nlu.load('en.pos.ud_ewt') returns Spark NLP model pos_ud_ewt\n","For language <fr> NLU provides the following Models : \n","nlu.load('fr.pos') returns Spark NLP model pos_ud_gsd\n","nlu.load('fr.pos.ud_gsd') returns Spark NLP model pos_ud_gsd\n","For language <de> NLU provides the following Models : \n","nlu.load('de.pos.ud_hdt') returns Spark NLP model pos_ud_hdt\n","nlu.load('de.pos') returns Spark NLP model pos_ud_hdt\n","For language <it> NLU provides the following Models : \n","nlu.load('it.pos') returns Spark NLP model pos_ud_isdt\n","nlu.load('it.pos.ud_isdt') returns Spark NLP model pos_ud_isdt\n","For language <nb> NLU provides the following Models : \n","nlu.load('nb.pos.ud_bokmaal') returns Spark NLP model pos_ud_bokmaal\n","For language <nn> NLU provides the following Models : \n","nlu.load('nn.pos') returns Spark NLP model pos_ud_nynorsk\n","nlu.load('nn.pos.ud_nynorsk') returns Spark NLP model pos_ud_nynorsk\n","For language <pl> NLU provides the following Models : \n","nlu.load('pl.pos') returns Spark NLP model pos_ud_lfg\n","nlu.load('pl.pos.ud_lfg') returns Spark NLP model pos_ud_lfg\n","For language <pt> NLU provides the following Models : \n","nlu.load('pt.pos.ud_bosque') returns Spark NLP model pos_ud_bosque\n","nlu.load('pt.pos') returns Spark NLP model pos_ud_bosque\n","For language <ru> NLU provides the following Models : \n","nlu.load('ru.pos.ud_gsd') returns Spark NLP model pos_ud_gsd\n","nlu.load('ru.pos') returns Spark NLP model pos_ud_gsd\n","For language <es> NLU provides the following Models : \n","nlu.load('es.pos') returns Spark NLP model pos_ud_gsd\n","nlu.load('es.pos.ud_gsd') returns Spark NLP model pos_ud_gsd\n","For language <ar> NLU provides the following Models : \n","nlu.load('ar.pos') returns Spark NLP model pos_ud_padt\n","For language <hy> NLU provides the following Models : \n","nlu.load('hy.pos') returns Spark NLP model pos_ud_armtdp\n","For language <eu> NLU provides the following Models : \n","nlu.load('eu.pos') returns Spark NLP model pos_ud_bdt\n","For language <bn> NLU provides the following Models : \n","nlu.load('bn.pos') returns Spark NLP model pos_msri\n","For language <br> NLU provides the following Models : \n","nlu.load('br.pos') returns Spark NLP model pos_ud_keb\n","For language <bg> NLU provides the following Models : \n","nlu.load('bg.pos') returns Spark NLP model pos_ud_btb\n","nlu.load('bg.pos.ud_btb') returns Spark NLP model pos_ud_btb\n","For language <ca> NLU provides the following Models : \n","nlu.load('ca.pos') returns Spark NLP model pos_ud_ancora\n","For language <cs> NLU provides the following Models : \n","nlu.load('cs.pos') returns Spark NLP model pos_ud_pdt\n","nlu.load('cs.pos.ud_pdt') returns Spark NLP model pos_ud_pdt\n","For language <fi> NLU provides the following Models : \n","nlu.load('fi.pos.ud_tdt') returns Spark NLP model pos_ud_tdt\n","nlu.load('fi.pos') returns Spark NLP model pos_ud_tdt\n","For language <gl> NLU provides the following Models : \n","nlu.load('gl.pos') returns Spark NLP model pos_ud_treegal\n","For language <el> NLU provides the following Models : \n","nlu.load('el.pos') returns Spark NLP model pos_ud_gdt\n","nlu.load('el.pos.ud_gdt') returns Spark NLP model pos_ud_gdt\n","For language <he> NLU provides the following Models : \n","nlu.load('he.pos') returns Spark NLP model pos_ud_htb\n","nlu.load('he.pos.ud_htb') returns Spark NLP model pos_ud_htb\n","For language <hi> NLU provides the following Models : \n","nlu.load('hi.pos') returns Spark NLP model pos_ud_hdtb\n","For language <hu> NLU provides the following Models : \n","nlu.load('hu.pos') returns Spark NLP model pos_ud_szeged\n","nlu.load('hu.pos.ud_szeged') returns Spark NLP model pos_ud_szeged\n","For language <id> NLU provides the following Models : \n","nlu.load('id.pos') returns Spark NLP model pos_ud_gsd\n","For language <ga> NLU provides the following Models : \n","nlu.load('ga.pos') returns Spark NLP model pos_ud_idt\n","For language <da> NLU provides the following Models : \n","nlu.load('da.pos') returns Spark NLP model pos_ud_ddt\n","For language <ja> NLU provides the following Models : \n","nlu.load('ja.pos') returns Spark NLP model pos_ud_gsd\n","nlu.load('ja.pos.ud_gsd') returns Spark NLP model pos_ud_gsd\n","For language <la> NLU provides the following Models : \n","nlu.load('la.pos') returns Spark NLP model pos_ud_llct\n","For language <lv> NLU provides the following Models : \n","nlu.load('lv.pos') returns Spark NLP model pos_ud_lvtb\n","For language <mr> NLU provides the following Models : \n","nlu.load('mr.pos') returns Spark NLP model pos_ud_ufal\n","For language <fa> NLU provides the following Models : \n","nlu.load('fa.pos') returns Spark NLP model pos_ud_perdt\n","For language <ro> NLU provides the following Models : \n","nlu.load('ro.pos') returns Spark NLP model pos_ud_rrt\n","nlu.load('ro.pos.ud_rrt') returns Spark NLP model pos_ud_rrt\n","For language <sk> NLU provides the following Models : \n","nlu.load('sk.pos') returns Spark NLP model pos_ud_snk\n","nlu.load('sk.pos.ud_snk') returns Spark NLP model pos_ud_snk\n","For language <sl> NLU provides the following Models : \n","nlu.load('sl.pos') returns Spark NLP model pos_ud_ssj\n","For language <sv> NLU provides the following Models : \n","nlu.load('sv.pos') returns Spark NLP model pos_ud_tal\n","nlu.load('sv.pos.ud_tal') returns Spark NLP model pos_ud_tal\n","For language <th> NLU provides the following Models : \n","nlu.load('th.pos') returns Spark NLP model pos_lst20\n","For language <tr> NLU provides the following Models : \n","nlu.load('tr.pos') returns Spark NLP model pos_ud_imst\n","nlu.load('tr.pos.ud_imst') returns Spark NLP model pos_ud_imst\n","For language <uk> NLU provides the following Models : \n","nlu.load('uk.pos') returns Spark NLP model pos_ud_iu\n","nlu.load('uk.pos.ud_iu') returns Spark NLP model pos_ud_iu\n","For language <yo> NLU provides the following Models : \n","nlu.load('yo.pos') returns Spark NLP model pos_ud_ytb\n","For language <zh> NLU provides the following Models : \n","nlu.load('zh.pos') returns Spark NLP model pos_ud_gsd\n","nlu.load('zh.pos.ud_gsd') returns Spark NLP model pos_ud_gsd\n","nlu.load('zh.pos.ctb9') returns Spark NLP model pos_ctb9\n","nlu.load('zh.pos.ud_gsd_trad') returns Spark NLP model pos_ud_gsd_trad\n","For language <et> NLU provides the following Models : \n","nlu.load('et.pos') returns Spark NLP model pos_ud_edt\n","For language <ur> NLU provides the following Models : \n","nlu.load('ur.pos') returns Spark NLP model pos_ud_udtb\n","nlu.load('ur.pos.ud_udtb') returns Spark NLP model pos_ud_udtb\n","For language <ko> NLU provides the following Models : \n","nlu.load('ko.pos') returns Spark NLP model pos_ud_kaist\n","nlu.load('ko.pos.ud_kaist') returns Spark NLP model pos_ud_kaist\n","For language <bh> NLU provides the following Models : \n","nlu.load('bh.pos') returns Spark NLP model pos_ud_bhtb\n","For language <am> NLU provides the following Models : \n","nlu.load('am.pos') returns Spark NLP model pos_ud_att\n"],"name":"stdout"}]}]}